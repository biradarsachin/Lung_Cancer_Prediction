# -*- coding: utf-8 -*-
"""Lung_Cancer_Project_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AmCIaveyjkWtHj6FYd7aysVB0cXcvuta
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded=files.upload()

data= pd.read_csv("cancer patient data sets.csv")
data

data_clean = data.drop(['index', 'Patient Id'], axis=1)

data_clean

data_clean.corr()

"""#Visualization of Correalation by heat map"""

import matplotlib.pyplot as plt
import seaborn as sns

# assume corr_matrix is already calculated
corr_matrix = data_clean.corr()


# set figure size
plt.figure(figsize=(17, 17))

# create heatmap
sns.heatmap(corr_matrix, annot=True, cmap="YlGnBu")

# add title
plt.title('Correlation Matrix Heatmap', fontsize=16)

# display plot
plt.show()

"""#Checking the value counts of features"""

data_clean['Air Pollution'].value_counts()

import matplotlib.pyplot as plt

# Get the value counts of the 'Air Pollution' column
air_pollution_counts = data_clean['Air Pollution'].value_counts()

# Create a pie chart
plt.pie(air_pollution_counts, labels=air_pollution_counts.index)

# Add a title
plt.title('Air Pollution')

# Display the chart
plt.show()

data_clean['Alcohol use'].value_counts()

import matplotlib.pyplot as plt

Alcohol_use_counts = data_clean['Alcohol use'].value_counts()

# Create a pie chart
plt.pie(Alcohol_use_counts, labels=Alcohol_use_counts.index)

# Add a title
plt.title('Alcohol use')

# Display the chart
plt.show()

data_clean['Dust Allergy'].value_counts()

import matplotlib.pyplot as plt

# Get the value counts of the 'Air Pollution' column
Dust_Allergy_counts = data_clean['Dust Allergy'].value_counts()

# Create a pie chart
plt.pie(Dust_Allergy_counts, labels=Dust_Allergy_counts.index)

# Add a title
plt.title('Dust Allergy')

# Display the chart
plt.show()

data['OccuPational Hazards'].value_counts()

import matplotlib.pyplot as plt

# Get the value counts of the 'OccuPational Hazards' column
OccuPational_Hazards_counts = data_clean['OccuPational Hazards'].value_counts()

# Create a pie chart
plt.pie(OccuPational_Hazards_counts, labels=OccuPational_Hazards_counts.index)

# Add a title
plt.title('OccuPational Hazards')

# Display the chart
plt.show()

data_clean['Genetic Risk'].value_counts()

import matplotlib.pyplot as plt

# Get the value counts of the 'Genetic Risk' column
Genetic_Risk_counts = data_clean['Genetic Risk'].value_counts()

# Create a pie chart
plt.pie(Genetic_Risk_counts, labels=Genetic_Risk_counts.index)

# Add a title
plt.title('Genetic Risk')

# Display the chart
plt.show()

data_clean['chronic Lung Disease'].value_counts()

import matplotlib.pyplot as plt

# Get the value counts of the 'chronic Lung Disease' column
chronic_Lung_Disease_counts = data_clean['chronic Lung Disease'].value_counts()

# Create a pie chart
plt.pie(chronic_Lung_Disease_counts, labels=chronic_Lung_Disease_counts.index)

# Add a title
plt.title('chronic Lung Disease')

# Display the chart
plt.show()

"""#Data Visualization"""

plt.figure(figsize=(15,10))
data.groupby('chronic Lung Disease')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of chronic Lung Disease on Lung cancer ')
plt.xlabel('chronic Lung Disease')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Air Pollution')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Air Pollution  on Lung cancer ')
plt.xlabel('Air Pollution ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Alcohol use')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Alcohol use   on Lung cancer ')
plt.xlabel('Alcohol use  ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Dust Allergy')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Dust Allergy   on Lung cancer ')
plt.xlabel(' Dust Allergy ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('OccuPational Hazards')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  OccuPational Hazards  on Lung cancer ')
plt.xlabel('OccuPational Hazards')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Genetic Risk')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Genetic Risk on Lung cancer ')
plt.xlabel('Genetic Risk ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Balanced Diet')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Balanced Diet on Lung cancer ')
plt.xlabel('Balanced Diet')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Obesity')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Obesity on Lung cancer ')
plt.xlabel('Obesity')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Smoking')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Smoking on Lung cancer ')
plt.xlabel('Smoking')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Passive Smoker')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Passive Smoker  on Lung cancer ')
plt.xlabel('Passive Smoker ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Chest Pain')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Chest Pain  on Lung cancer ')
plt.xlabel('Chest Pain')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Coughing of Blood')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Coughing of Blood   on Lung cancer ')
plt.xlabel('Coughing of Blood  ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Fatigue')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Fatigue on Lung cancer ')
plt.xlabel('Fatigue')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Weight Loss')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Weight Loss  on Lung cancer ')
plt.xlabel('Weight Loss ')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data.groupby('Shortness of Breath')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Shortness of Breath on Lung cancer ')
plt.xlabel('Shortness of Breath')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data_clean.groupby('Wheezing')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Wheezing   on Lung cancer ')
plt.xlabel('Wheezing')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data_clean.groupby('Swallowing Difficulty')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Swallowing Difficulty on Lung cancer ')
plt.xlabel('Swallowing Difficulty')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data_clean.groupby('Clubbing of Finger Nails')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Clubbing of Finger Nails on Lung cancer ')
plt.xlabel('Clubbing of Finger Nails')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data_clean.groupby('Frequent Cold')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Frequent Cold on Lung cancer ')
plt.xlabel('Frequent Cold')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data_clean.groupby('Dry Cough')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of  Dry Cough on Lung cancer ')
plt.xlabel('Dry Cough')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(15,10))
data_clean.groupby('Snoring')['Level'].value_counts().unstack().plot(kind='bar')
plt.title('Effect of Snoring  on Lung cancer ')
plt.xlabel('Snoring')
plt.ylabel('Frequency')
plt.show()

"""#Outlier Detection through box plot"""

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data = data_clean['Dry Cough']

# Create a box plot
plt.boxplot(data)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data')
plt.ylabel('Values')

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data = data_clean['Snoring']

# Create a box plot
plt.boxplot(data)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data_clean')
plt.ylabel('Values')

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data = data_clean['Frequent Cold']

# Create a box plot
plt.boxplot(data)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data_clean')
plt.ylabel('Values')

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data_clean2 = data_clean['Clubbing of Finger Nails']

# Create a box plot
plt.boxplot(data_clean2)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data_clean')
plt.ylabel('Values')

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data_clean2 = data_clean['Swallowing Difficulty']

# Create a box plot
plt.boxplot(data_clean2)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data_clean')
plt.ylabel('Values')

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data_clean2 = data_clean['Wheezing']

# Create a box plot
plt.boxplot(data_clean2)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data_clean')
plt.ylabel('Values')

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Select a column from the DataFrame containing numerical values
data_clean2 = data_clean['Shortness of Breath']

# Create a box plot
plt.boxplot(data_clean2)

# Add a title and axis labels
plt.title('Box Plot Example')
plt.xlabel('Data_clean')
plt.ylabel('Values')

# Display the plot
plt.show()

data_clean['Level'].value_counts()

import matplotlib.pyplot as plt

# Get the value counts of the 'Level' column
Level_counts = data_clean['Level'].value_counts()

# Create a pie chart
plt.pie(Level_counts, labels=Level_counts.index)

# Add a title
plt.title('Level')

# Display the chart
plt.show()

"""#Model Building"""

from sklearn.model_selection import train_test_split

# split the data into independent variables X and dependent variable y
X = data_clean.drop('Level', axis=1)
y = data_clean['Level']

# split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""#Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# create logistic regression object
logreg = LogisticRegression()

# fit the logistic regression model on the training data
logreg.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = logreg.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', logreg.score(X_test, y_test))

from sklearn.metrics import classification_report
# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#KNN"""

from sklearn.neighbors import KNeighborsClassifier

# create KNN object with k=5 neighbors
knn = KNeighborsClassifier(n_neighbors=5)

# fit the KNN model on the training data
knn.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = knn.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', knn.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# create decision tree object with maximum depth of 3
dt = DecisionTreeClassifier(max_depth=3)

# fit the decision tree model on the training data
dt.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = dt.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', dt.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

# create Naive Bayes object
nb = GaussianNB()

# fit the Naive Bayes model on the training data
nb.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = nb.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', nb.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#Random forest"""

from sklearn.ensemble import RandomForestClassifier

# create Random Forest object with 100 trees
rf = RandomForestClassifier(n_estimators=60)

# fit the Random Forest model on the training data
rf.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = rf.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', rf.score(X_test, y_test))

# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

# print the accuracy score of the model
print('Accuracy score:', knn.score(X_test, y_test))

"""#Support Vector Classifier using linear kernel"""

from sklearn.svm import SVC

# Create an SVM object
svm = SVC(kernel='linear', random_state=42)

# Fit the SVM model to the training data
svm.fit(X_train, y_train)

# Predict the target variable for the test data
y_pred = svm.predict(X_test)

# Print the accuracy of the model on the test data
print('Accuracy:', svm.score(X_test, y_test))

"""#Support Vector Classifier using RBF Kernal"""

from sklearn.svm import SVC

# create SVM object
svm = SVC(kernel='rbf', random_state=42)

# fit the SVM model on the training data
svm.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = svm.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', svm.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# create decision tree object
dt = DecisionTreeClassifier(max_depth=1, random_state=42)

# create AdaBoost object
adaboost = AdaBoostClassifier(base_estimator=dt, n_estimators=100, learning_rate=0.1, random_state=42)

# fit the AdaBoost model on the training data
adaboost.fit(X_train, y_train)

# predict the target variable for the training and test data
y_train_pred = adaboost.predict(X_train)
y_test_pred = adaboost.predict(X_test)

# print the training and test accuracy of the model
print('Training accuracy:', adaboost.score(X_train, y_train))
print('Test accuracy:', adaboost.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report for the test data
report = classification_report(y_test, y_test_pred)

print('Classification Report:\n', report)

"""#Renaming the columns"""

# rename the 'name' column to 'first_name'
df = data_clean.rename(columns={
    'Coughing of Blood': 'Coughing_of_Blood',
    'Air Pollution': ' Air_Pollution',
    'Alcohol use': 'Alcohol_use',
    'Dust Allergy': 'Dust_Allergy',
    'OccuPational Hazards': 'OccuPational_Hazards', 
    'Genetic Risk ': 'Genetic_Risk',
    'chronic Lung Disease': 'chronic_Lung_Disease',
     'Dry Cough': 'Dry_Cough',
     'Frequent Cold': 'Frequent_Cold',
     ' Clubbing of Finger Nails ': 'Clubbing_of_Finger_Nails',
     'Swallowing Difficulty':'Swallowing_Difficulty',
     'Shortness of Breath':'Shortness_of_Breath',
     'Weight Loss ':'Weight_Loss',
      'Passive Smoker':'Passive_Smoker',
      'Chest Pain':'Chest_Pain',
      'Balanced Diet':'Balanced_Diet'})

"""#Checking the multicollinearity between  features with VIF (Variation Inflation Factor)"""

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Drop any non-numeric columns that you don't want to include in the VIF calculation
X = df.select_dtypes(include=['float64', 'int64']).dropna()

# Calculate VIF for each variable
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif["features"] = X.columns

print(vif)

"""#Dropping columns with high VIF Values/highly correlated (Feature Engineering)"""

data_cleaned = data_clean.drop(['OccuPational Hazards','Genetic Risk','chronic Lung Disease','Dust Allergy','Alcohol use'], axis=1)

data_cleaned.columns

"""#Train Test split"""

# split the data into independent variables X and dependent variable y
X = data_cleaned.drop('Level', axis=1)
y = data_cleaned['Level']

"""#Normalization"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# split the data into independent variables X and dependent variable y
X = data_cleaned.drop('Level', axis=1)
y = data_cleaned['Level']

# split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create a MinMaxScaler object
scaler = MinMaxScaler()

# fit the scaler to the training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# transform the test data using the fitted scaler
X_test_scaled = scaler.transform(X_test)

"""#Model Building after Feature Engineering and Normalization

#Logistic regression
"""

from sklearn.linear_model import LogisticRegression

# create logistic regression object
logreg = LogisticRegression()

# fit the logistic regression model on the training data
logreg.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = logreg.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', logreg.score(X_test, y_test))

from sklearn.metrics import classification_report
# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

# create a DecisionTreeClassifier object
dt = DecisionTreeClassifier()

# fit the classifier to the scaled training data
dt.fit(X_train_scaled, y_train)

# predict on the scaled test data
y_pred = dt.predict(X_test_scaled)

# evaluate the accuracy of the model on the test data
accuracy = dt.score(X_test_scaled, y_test)
print("Accuracy:", accuracy)

from sklearn.metrics import classification_report

# get the classification report on the test data
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

# evaluate the accuracy of the model on the test data
accuracy = dt.score(X_test_scaled, y_test)
print("Test Accuracy:", accuracy)

"""#Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

# create Naive Bayes object
nb = GaussianNB()

# fit the Naive Bayes model on the training data
nb.fit(X_train, y_train)

# predict the target variable for the test data
y_pred = nb.predict(X_test)

# print the accuracy score of the model
print('Accuracy score:', nb.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report
report = classification_report(y_test, y_pred)

print('Classification Report:\n', report)

"""#Support Vector Machine"""

from sklearn.svm import SVC

# create SVM object
svm = SVC(kernel='rbf', random_state=42)

# fit the SVM model on the training data
svm.fit(X_train, y_train)

# predict the target variable for the training and test data
y_train_pred = svm.predict(X_train)
y_test_pred = svm.predict(X_test)

# print the training and test accuracy of the model
print('Training accuracy:', svm.score(X_train, y_train))
print('Test accuracy:', svm.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report for the test data
report = classification_report(y_test, y_test_pred)

print('Classification Report:\n', report)

"""#AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# create decision tree object
dt = DecisionTreeClassifier(max_depth=1, random_state=42)

# create AdaBoost object
adaboost = AdaBoostClassifier(base_estimator=dt, n_estimators=100, learning_rate=0.1, random_state=42)

# fit the AdaBoost model on the training data
adaboost.fit(X_train, y_train)

# predict the target variable for the training and test data
y_train_pred = adaboost.predict(X_train)
y_test_pred = adaboost.predict(X_test)

# print the training and test accuracy of the model
print('Training accuracy:', adaboost.score(X_train, y_train))
print('Test accuracy:', adaboost.score(X_test, y_test))

from sklearn.metrics import classification_report

# print classification report for the test data
report = classification_report(y_test, y_test_pred)

print('Classification Report:\n', report)

"""# Isolation forest to Detect outliers"""

from sklearn.ensemble import IsolationForest

# create an Isolation Forest object
iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)

# fit the model on the data
iso_forest.fit(X_train)

# predict the labels of the data
y_pred_train = iso_forest.predict(X_train)
y_pred_test = iso_forest.predict(X_test)

# print the number of outliers detected in the training and test data
print('Number of outliers in training data:', sum(y_pred_train == -1))
print('Number of outliers in test data:', sum(y_pred_test == -1))

train_outliers = np.where(y_pred_train == -1)[0]
test_outliers = np.where(y_pred_test == -1)[0]

print('Indices of outliers in training data:', train_outliers)
print('Indices of outliers in test data:', test_outliers)

train_data = pd.concat([X_train, y_train], axis=1)

train_outliers = np.where(y_pred_train == -1)[0]

train_data_clean = train_data.drop(train_data.index[train_outliers])

"""#Dropping The outlier"""

X_train_clean = train_data_clean.drop('Level', axis=1)
y_train_clean = train_data_clean['Level']

test_data = pd.concat([X_test, y_test], axis=1)
test_outliers = np.where(y_pred_test == -1)[0]
test_data_clean = test_data.drop(test_data.index[test_outliers])
X_test_clean = test_data_clean.drop('Level', axis=1)
y_test_clean = test_data_clean['Level']

"""#Model Building after Dropping the ouliers

#Adaboost
"""

from sklearn.ensemble import AdaBoostClassifier

# create an AdaBoostClassifier object
ada_boost = AdaBoostClassifier(n_estimators=100, random_state=42)

# fit the model on the cleaned training data
ada_boost.fit(X_train_clean, y_train_clean)

# predict the labels of the cleaned test data
y_pred = ada_boost.predict(X_test_clean)

# evaluate the accuracy of the model on the cleaned test data
accuracy = ada_boost.score(X_test_clean, y_test_clean)
print('Accuracy:', accuracy)

"""Achieved an accuracy of 74% on normalization, dropping outliers, Feature Engineering

#Hyperparameter tuning to obtain best value of n_estimator and learning rate
"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

# create a decision tree object with max_depth=2
dt = DecisionTreeClassifier(max_depth=2, random_state=42)

# create an AdaBoostClassifier object with dt as the base estimator
ada_boost = AdaBoostClassifier(base_estimator=dt, random_state=42)

# define the grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'learning_rate': [0.01, 0.1, 1]
}

# create a GridSearchCV object
grid_search = GridSearchCV(ada_boost, param_grid=param_grid, cv=5)

# fit the GridSearchCV object on the cleaned training data
grid_search.fit(X_train_clean, y_train_clean)

# print the best hyperparameters found
print('Best hyperparameters:', grid_search.best_params_)

"""#Fitting Adaboost model with optimum values of hyperparameters  obtained through hyperparameter tuning

#AdaBoost
"""

from sklearn.ensemble import AdaBoostClassifier

# create an AdaBoostClassifier object with n_estimators=100 and learning_rate=0.01
ada_boost = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=42)

# fit the model on the cleaned training data
ada_boost.fit(X_train_clean, y_train_clean)

# predict the labels of the cleaned test data
y_pred = ada_boost.predict(X_test_clean)

# evaluate the accuracy of the model on the cleaned test data
accuracy = ada_boost.score(X_test_clean, y_test_clean)
print('Accuracy:', accuracy)

"""Accuracy of model increased from 73% to 93%

#Decision Tree
"""

from sklearn.tree import DecisionTreeClassifier

# create a DecisionTreeClassifier object
decision_tree = DecisionTreeClassifier(random_state=42)

# fit the model on the cleaned training data
decision_tree.fit(X_train_clean, y_train_clean)

# predict the labels of the cleaned test data
y_pred = decision_tree.predict(X_test_clean)

# evaluate the accuracy of the model on the cleaned test data
accuracy = decision_tree.score(X_test_clean, y_test_clean)
print('Accuracy:', accuracy)

"""#Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

# create a Gaussian Naive Bayes object
nb = GaussianNB()

# fit the model on the cleaned data
nb.fit(X_train_clean, y_train_clean)

# predict the labels of the cleaned test data
y_pred_test = nb.predict(X_test_clean)

from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, classification_report

# calculate the accuracy on the cleaned test data
accuracy = accuracy_score(y_test_clean, y_pred_test)
print("Accuracy:", accuracy)

# print the classification report
print("Classification Report:\n", classification_report(y_test_clean, y_pred_test))

"""#In this Project i mainly focused on Adaboost model and improved its accuracy from 69% to 93%"""